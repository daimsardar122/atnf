{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyMFA4UeH/pMOASOxAJMe1gi"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"iIUwwJb3iBz1"},"outputs":[],"source":["import astropy  # Import the core package for Astronomy\n","from astropy.table import Table, Column, MaskedColumn  # Import table classes for handling data\n","from astropy.coordinates import SkyCoord  # Import coordinate handling class\n","import numpy as np  # Import numpy for numerical operations\n","import matplotlib.pyplot as plt  # Import matplotlib for plotting\n","import seaborn as sns  # Import seaborn for advanced plotting\n","import urllib.request  # Import urllib for handling URL requests\n","import tarfile  # Import tarfile for extracting tar.gz files\n","from astropy import units as u  # Import units for handling astronomical units\n","\n","# Download and extract the tar.gz file\n","pulsargzfile = urllib.request.urlopen('http://www.atnf.csiro.au/people/pulsar/psrcat/downloads/psrcat_pkg.tar.gz')  # Download the file\n","outputgzfile = open('psrcat_pkg.tar.gz', 'wb')  # Open a file to write the downloaded data\n","outputgzfile.write(pulsargzfile.read())  # Write the downloaded data to the file\n","outputgzfile.close()  # Close the file\n","pulsargzfile.close()  # Close the URL request\n","\n","pulsargz = tarfile.open('psrcat_pkg.tar.gz', 'r')  # Open the downloaded tar.gz file\n","pulsargz.extractall()  # Extract all contents of the tar.gz file\n","pulsargz.close()  # Close the tar.gz file\n","\n","# Function to determine data types\n","def getType(value):\n","    tests = [\n","        (float, float),  # Test if the value can be converted to a float\n","        (int, int),  # Test if the value can be converted to an int\n","        (str, lambda value: value.strip())  # Treat value as a string and strip whitespace\n","    ]\n","    for typ, test in tests:\n","        try:\n","            test(value)\n","            return typ  # Return the type if the conversion is successful\n","        except ValueError:\n","            continue\n","    # No match found, return string type\n","    return str\n","\n","# Read and process the database file\n","psrcatdb = 'psrcat_tar/psrcat.db'  # Path to the database file\n","breakstring = '@-----------------------------------------------------------------'  # Delimiter for new entries\n","commentstring = '#'  # Comment identifier\n","versionstring = '#CATALOGUE'  # Version identifier\n","\n","datafile = open(psrcatdb)  # Open the database file\n","psrtable = Table(masked=True)  # Create a masked table to handle missing data\n","ind = 0  # Index for the rows\n","masking = []  # List to keep track of masked values\n","\n","for line in datafile:\n","    dataline = line.split()  # Split the line into components\n","    if dataline[0] == breakstring or dataline[0] == commentstring or dataline[0] == versionstring:\n","        if dataline[0] == breakstring:\n","            psrtable.add_row(None)  # Add a new row for each entry\n","            ind += 1  # Increment the row index\n","            psrtable.mask[ind] = masking  # Apply the mask to the new row\n","        continue\n","\n","    if dataline[0] not in psrtable.colnames:\n","        masking.append(True)  # Add a mask for the new column\n","        thisdatatype = getType(dataline[1])  # Determine the data type of the column\n","        if thisdatatype == float:\n","            thisdtstr = 'f4'  # 4-byte floating point\n","        elif thisdatatype == int:\n","            thisdtstr = 'i2'  # 2-byte integer\n","        else:\n","            thisdtstr = 'S100'  # String of maximum length 100\n","        newcolumn = MaskedColumn(name=dataline[0], dtype=thisdtstr, mask=True, length=ind + 1)  # Create a new column with the appropriate type and mask\n","        psrtable.add_column(newcolumn)  # Add the new column to the table\n","\n","    psrtable[dataline[0]][ind] = dataline[1]  # Assign the data value to the appropriate cell\n","    psrtable[dataline[0]].mask[ind] = False  # Unmask the cell\n","\n","datafile.close()  # Close the database file\n","\n","# Convert frequencies and derivatives into periods and derivatives\n","for obj in range(len(psrtable)):\n","    if not psrtable['F0'].mask[obj]:  # Check if F0 is not masked\n","        if psrtable['P0'].mask[obj]:  # Check if P0 is masked\n","            psrtable['P0'].mask[obj] = False  # Unmask P0\n","            psrtable['P0'][obj] = 1.0 / psrtable['F0'][obj]  # Calculate the period from the frequency\n","        if not psrtable['F1'].mask[obj]:  # Check if F1 is not masked\n","            if psrtable['P1'].mask[obj]:  # Check if P1 is masked\n","                psrtable['P1'].mask[obj] = False  # Unmask P1\n","                psrtable['P1'][obj] = (-1.0 * psrtable['F1'][obj]) / (psrtable['F0'][obj] ** 2)  # Calculate the period derivative\n","\n","# Ensure RAJ and DECJ columns are strings and correctly formatted\n","psrtable['RAJ'] = psrtable['RAJ'].astype(str)  # Convert RAJ column to string type\n","psrtable['DECJ'] = psrtable['DECJ'].astype(str)  # Convert DECJ column to string type\n","\n","# Convert coordinates from string to SkyCoord\n","psrtable['coord'] = SkyCoord(psrtable['RAJ'], psrtable['DECJ'], unit=(u.hourangle, u.deg))  # Convert RAJ and DECJ to SkyCoord\n","\n","# Plotting\n","# 1. Aitoff projection for Galactic coordinates\n","fig = plt.figure(figsize=(10, 5))  # Create a figure\n","ax = plt.subplot(111, projection='aitoff')  # Create an Aitoff projection subplot\n","ax.grid(True)  # Enable the grid\n","ax.plot(-1 * psrtable['coord'].galactic.l.radian, psrtable['coord'].galactic.b.radian, '.', color='0.5')  # Plot the Galactic coordinates\n","ax.plot(2 * np.pi - psrtable['coord'].galactic.l.radian, psrtable['coord'].galactic.b.radian, '.', color='0.5')  # Mirror the coordinates for the Aitoff projection\n","ax.set_xticklabels([\n","    r'$150^\\circ$', r'$120^\\circ$', r'$90^\\circ$', r'$60^\\circ$',\n","    r'$30^\\circ$', r'$0^\\circ$', r'$330^\\circ$', r'$300^\\circ$',\n","    r'$270^\\circ$', r'$240^\\circ$', r'$210^\\circ$'\n","])  # Set the x-axis labels\n","plt.ylabel(r'$b$', fontsize=12)  # Set the y-axis label\n","plt.show()  # Show the plot\n","\n","# 2. Aitoff projection for Equatorial coordinates\n","fig = plt.figure(figsize=(10, 5))  # Create a figure\n","ax = plt.subplot(111, projection='aitoff')  # Create an Aitoff projection subplot\n","ax.grid(True)  # Enable the grid\n","ax.plot(-1 * psrtable['coord'].ra.radian, psrtable['coord'].dec.radian, '.', color='0.5')  # Plot the Equatorial coordinates\n","ax.plot(2 * np.pi - psrtable['coord'].ra.radian, psrtable['coord'].dec.radian, '.', color='0.5')  # Mirror the coordinates for the Aitoff projection\n","ax.set_xticklabels([\n","    r'$10^{\\rm h}$', r'$8^{\\rm h}$', r'$6^{\\rm h}$', r'$4^{\\rm h}$',\n","    r'$2^{\\rm h}$', r'$0^{\\rm h}$', r'$22^{\\rm h}$', r'$20^{\\rm h}$',\n","    r'$18^{\\rm h}$', r'$16^{\\rm h}$', r'$14^{\\rm h}$'\n","])  # Set the x-axis labels\n","plt.ylabel(r'Dec', fontsize=16)  # Set the y-axis label\n","plt.show()  # Show the plot\n","\n","# 3. P-Pdot diagram\n","fig = plt.figure()  # Create a figure\n","ax = plt.gca()  # Get the current axes\n","ax.plot(psrtable['P0'], psrtable['P1'], '.', c='black')  # Plot P0 vs P1\n","ax.set_yscale('log')  # Set y-axis to log scale\n","ax.set_xscale('log')  # Set x-axis to log scale\n","ax.plot(psrtable['P0'][psrtable['BINARY'].mask == False],\n","        psrtable['P1'][psrtable['BINARY'].mask == False], 'o', mfc='None')  # Highlight binary pulsars\n","ax.plot([0.0328449, 100], [1.0E-21, 2.8223E-11], color='k', linestyle='-', linewidth=2)  # Plot a reference line\n","ax.plot([1.E-3, 1.E2], [1.1E-19, 5.1E-13], color='k', linestyle='--', linewidth=2)  # Plot another reference line\n","plt.xlabel(r'$P\\ {\\rm(s)}$', fontsize=16)  # Set the x-axis label\n","plt.ylabel(r'$\\dot{P}\\ {\\rm(s\\ s^{-1})}$', fontsize=16)  # Set the y-axis label\n","plt.show()  # Show the plot\n","\n","# 4. Distribution plot using seaborn\n","sns.histplot(psrtable['F0'].compressed(), bins=20, kde=True)  # Create a histogram of F0 with KDE using seaborn\n","plt.xlabel('F0')  # Set the x-axis label\n","plt.ylabel('Frequency')  # Set the y-axis label\n","plt.title('Distribution of F0')  # Set the plot title\n","plt.show()  # Show the plot"]},{"cell_type":"markdown","source":["### Explanation of the First Cell\n","\n","The first cell of the code performs the following tasks:\n","\n","1. **Import Libraries:**\n","   - Imports necessary libraries for handling astronomical data, manipulating arrays, and plotting.\n","\n","2. **Download and Extract Data:**\n","   - Downloads the pulsar catalog data from the ATNF website and extracts the contents from a tar.gz file.\n","\n","3. **Define Helper Function:**\n","   - Defines a function to determine the data type of values in the catalog.\n","\n","4. **Read and Process Database File:**\n","   - Reads the pulsar catalog file and creates a masked table to handle missing values.\n","   - Processes each line of the file to populate the table, handling comments and version strings.\n","\n","5. **Convert Frequencies and Derivatives:**\n","   - Converts frequency (`F0`) and its derivative (`F1`) to period (`P0`) and its derivative (`P1`) for the pulsars.\n","\n","6. **Format Coordinates:**\n","   - Ensures the Right Ascension (`RAJ`) and Declination (`DECJ`) columns are strings and converts them to `SkyCoord` objects for easy handling and plotting of celestial coordinates.\n","\n","7. **Plot Data:**\n","   - Creates various plots:\n","     - **Aitoff Projection for Galactic Coordinates:** Shows the distribution of pulsars in the Galactic coordinate system.\n","     - **Aitoff Projection for Equatorial Coordinates:** Shows the distribution of pulsars in the Equatorial coordinate system.\n","     - **P-Pdot Diagram:** Plots the period against the period derivative, a key diagnostic tool for studying pulsar evolution.\n","     - **Distribution Plot:** Visualizes the distribution of the spin frequency (`F0`) of pulsars using seaborn.\n","\n","This cell sets up the dataset and provides initial visual insights into the data, which are essential steps for understanding the structure and distribution of pulsars before applying machine learning techniques."],"metadata":{"id":"N_uUSRvhi5pW"}},{"cell_type":"code","source":["from sklearn.preprocessing import StandardScaler  # Import the StandardScaler for data standardization\n","from sklearn.decomposition import PCA  # Import PCA for dimensionality reduction\n","from sklearn.cluster import KMeans  # Import KMeans for clustering\n","from sklearn.metrics import silhouette_score  # Import silhouette_score for evaluating cluster quality\n","import matplotlib.pyplot as plt  # Import matplotlib for plotting\n","import seaborn as sns  # Import seaborn for enhanced visualization\n","\n","# Extract relevant features and drop rows with missing values\n","features = ['P0', 'P1', 'F0', 'F1']  # Define the relevant features to be used\n","data = psrtable[features].to_pandas()  # Convert the relevant columns of the table to a pandas DataFrame\n","data = data.dropna()  # Drop rows with missing values\n","\n","# Standardize the data\n","scaler = StandardScaler()  # Initialize the StandardScaler\n","data_scaled = scaler.fit_transform(data)  # Fit the scaler on the data and transform it\n","\n","# Dimensionality reduction for visualization\n","pca = PCA(n_components=2)  # Initialize PCA to reduce data to 2 components for visualization\n","data_pca = pca.fit_transform(data_scaled)  # Fit PCA on the scaled data and transform it\n","\n","# K-Means Clustering\n","kmeans = KMeans(n_clusters=4, random_state=42)  # Initialize KMeans with 4 clusters and a fixed random state\n","clusters = kmeans.fit_predict(data_scaled)  # Fit KMeans on the scaled data and predict cluster assignments\n","\n","# Evaluation of Clusters\n","silhouette_avg = silhouette_score(data_scaled, clusters)  # Calculate the silhouette score to evaluate clustering\n","print(f'Silhouette Score: {silhouette_avg}')  # Print the silhouette score\n","\n","# Visualization of Clusters\n","plt.figure(figsize=(10, 7))  # Create a figure for the plot with specified size\n","sns.scatterplot(x=data_pca[:, 0], y=data_pca[:, 1], hue=clusters, palette='viridis', s=100, alpha=0.7)  # Create a scatter plot with PCA components, colored by clusters\n","plt.title('K-Means Clustering of Neutron Stars')  # Set the plot title\n","plt.xlabel('PCA Component 1')  # Set the x-axis label\n","plt.ylabel('PCA Component 2')  # Set the y-axis label\n","plt.show()  # Show the plot"],"metadata":{"id":"YpJ8eBtuicli"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Explanation of the Second Cell\n","\n","The second cell of the code focuses on clustering the neutron stars using K-Means clustering. Here is a step-by-step breakdown:\n","\n","1. **Import Libraries:**\n","   - Imports necessary libraries for preprocessing, dimensionality reduction, clustering, and visualization.\n","\n","2. **Extract Relevant Features:**\n","   - Selects relevant features (`P0`, `P1`, `F0`, `F1`) from the `psrtable` and converts it to a pandas DataFrame.\n","   - Drops rows with missing values to ensure the data is complete for analysis.\n","\n","3. **Standardize the Data:**\n","   - Uses `StandardScaler` to standardize the data, ensuring each feature contributes equally to the clustering process.\n","\n","4. **Dimensionality Reduction:**\n","   - Applies Principal Component Analysis (PCA) to reduce the dimensionality of the data to 2 components for visualization purposes.\n","\n","5. **K-Means Clustering:**\n","   - Applies K-Means clustering to the standardized data, specifying 4 clusters and a random state for reproducibility.\n","\n","6. **Evaluate Clusters:**\n","   - Computes the silhouette score to evaluate the quality of the clusters. A higher silhouette score indicates better-defined clusters.\n","\n","7. **Visualize Clusters:**\n","   - Creates a scatter plot of the PCA-reduced data, coloring points by their cluster assignments. This helps visualize the separation and cohesion of the clusters.\n","\n","This cell preprocesses the data, applies K-Means clustering, evaluates the clustering performance, and visualizes the clusters, providing insights into the structure and grouping of neutron stars based on the selected features."],"metadata":{"id":"rAnElUSCi8z5"}},{"cell_type":"code","source":["from scipy.cluster.hierarchy import linkage, dendrogram, fcluster  # Import functions for hierarchical clustering and visualization\n","\n","# Hierarchical Clustering\n","Z = linkage(data_scaled, method='ward')  # Perform hierarchical clustering using Ward's method\n","hier_clusters = fcluster(Z, t=4, criterion='maxclust')  # Form flat clusters from the hierarchical tree with 4 clusters\n","\n","# Evaluation of Hierarchical Clusters\n","silhouette_avg_hier = silhouette_score(data_scaled, hier_clusters)  # Calculate the silhouette score for hierarchical clusters\n","print(f'Hierarchical Clustering Silhouette Score: {silhouette_avg_hier}')  # Print the silhouette score\n","\n","# Visualization of Hierarchical Clusters\n","plt.figure(figsize=(10, 7))  # Set the size of the figure\n","sns.scatterplot(x=data_pca[:, 0], y=data_pca[:, 1], hue=hier_clusters, palette='viridis', s=100, alpha=0.7)  # Create a scatter plot of PCA components with hierarchical cluster labels\n","plt.title('Hierarchical Clustering of Neutron Stars')  # Set the title of the plot\n","plt.xlabel('PCA Component 1')  # Label the x-axis\n","plt.ylabel('PCA Component 2')  # Label the y-axis\n","plt.show()  # Display the plot\n","\n","# Dendrogram\n","plt.figure(figsize=(12, 8))  # Set the size of the figure\n","dendrogram(Z)  # Plot the dendrogram based on the linkage matrix\n","plt.title('Dendrogram for Hierarchical Clustering')  # Set the title of the dendrogram\n","plt.xlabel('Sample Index')  # Label the x-axis\n","plt.ylabel('Distance')  # Label the y-axis\n","plt.show()  # Display the dendrogram"],"metadata":{"id":"Y0mNUj80jSEK"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Explanation of the Third Cell\n","\n","The third cell focuses on performing hierarchical clustering on the neutron star data. Here's a breakdown of its functionality:\n","\n","1. **Import Libraries:**\n","   - Imports necessary libraries for hierarchical clustering and visualization.\n","\n","2. **Hierarchical Clustering:**\n","   - Performs hierarchical clustering on the standardized data using the Ward's method, which minimizes the variance within clusters.\n","   - Generates a linkage matrix `Z` that encodes the hierarchical clustering structure.\n","   - Assigns cluster labels to the data by cutting the hierarchical tree at a height that forms 4 clusters.\n","\n","3. **Evaluate Hierarchical Clusters:**\n","   - Computes the silhouette score to evaluate the quality of the hierarchical clusters. A higher silhouette score indicates better-defined clusters.\n","\n","4. **Visualize Hierarchical Clusters:**\n","   - Creates a scatter plot of the PCA-reduced data, coloring points by their hierarchical cluster assignments. This helps visualize the separation and cohesion of the clusters.\n","\n","5. **Dendrogram:**\n","   - Plots a dendrogram using the linkage matrix `Z`, which shows the hierarchical relationships between the data points. The dendrogram illustrates how clusters are merged at each step of the hierarchical clustering process.\n","\n","This cell implements hierarchical clustering, evaluates the clustering performance, and visualizes the clusters and the hierarchical structure, providing additional insights into the grouping of neutron stars."],"metadata":{"id":"Vr6Xkf4fjio3"}},{"cell_type":"code","source":["from sklearn.cluster import DBSCAN  # Import the DBSCAN clustering algorithm from scikit-learn\n","\n","# DBSCAN Clustering\n","dbscan = DBSCAN(eps=0.5, min_samples=5)  # Initialize the DBSCAN algorithm with epsilon (eps) set to 0.5 and minimum samples (min_samples) set to 5\n","dbscan_clusters = dbscan.fit_predict(data_scaled)  # Fit the DBSCAN model to the scaled data and predict the cluster labels\n","\n","# Evaluation of DBSCAN Clusters\n","silhouette_avg_dbscan = silhouette_score(data_scaled, dbscan_clusters)  # Calculate the silhouette score to evaluate the quality of the DBSCAN clusters\n","print(f'DBSCAN Silhouette Score: {silhouette_avg_dbscan}')  # Print the silhouette score for DBSCAN clustering\n","\n","# Visualization of DBSCAN Clusters\n","plt.figure(figsize=(10, 7))  # Set the size of the figure for the plot\n","sns.scatterplot(x=data_pca[:, 0], y=data_pca[:, 1], hue=dbscan_clusters, palette='viridis', s=100, alpha=0.7)  # Create a scatter plot of PCA components with DBSCAN cluster labels\n","plt.title('DBSCAN Clustering of Neutron Stars')  # Set the title of the scatter plot\n","plt.xlabel('PCA Component 1')  # Label the x-axis of the plot\n","plt.ylabel('PCA Component 2')  # Label the y-axis of the plot\n","plt.show()  # Display the scatter plot"],"metadata":{"id":"mfOjJbiKjq7W"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["\n","### Explanation\n","\n","1. **Importing DBSCAN**: The `DBSCAN` algorithm is imported from scikit-learn, which is used for density-based clustering.\n","\n","2. **DBSCAN Clustering**:\n","   - `dbscan = DBSCAN(eps=0.5, min_samples=5)`: The `DBSCAN` algorithm is initialized with `eps` (maximum distance between samples to be considered neighbors) set to 0.5 and `min_samples` (minimum number of samples in a neighborhood for a point to be considered as a core point) set to 5.\n","   - `dbscan_clusters = dbscan.fit_predict(data_scaled)`: The `fit_predict` method applies the DBSCAN algorithm to the scaled data, assigning each data point to a cluster and returning an array of cluster labels.\n","\n","3. **Evaluation of DBSCAN Clusters**:\n","   - `silhouette_avg_dbscan = silhouette_score(data_scaled, dbscan_clusters)`: The silhouette score, which measures how similar each point is to its own cluster compared to other clusters, is computed to evaluate the quality of clustering.\n","   - `print(f'DBSCAN Silhouette Score: {silhouette_avg_dbscan}')`: The silhouette score is printed to provide an assessment of the clustering performance.\n","\n","4. **Visualization of DBSCAN Clusters**:\n","   - `plt.figure(figsize=(10, 7))`: The size of the figure for the plot is set to 10 by 7 inches.\n","   - `sns.scatterplot(x=data_pca[:, 0], y=data_pca[:, 1], hue=dbscan_clusters, palette='viridis', s=100, alpha=0.7)`: A scatter plot of the PCA-transformed data is created with points colored according to their DBSCAN cluster labels. The `hue` parameter is set to `dbscan_clusters` to use the cluster labels for coloring, `palette='viridis'` specifies the color map, `s=100` sets the size of the points, and `alpha=0.7` sets the transparency.\n","   - `plt.title('DBSCAN Clustering of Neutron Stars')`: The title of the scatter plot is set.\n","   - `plt.xlabel('PCA Component 1')` and `plt.ylabel('PCA Component 2')`: Labels for the x and y axes are set.\n","   - `plt.show()`: Displays the scatter plot, showing the clustering result."],"metadata":{"id":"oJb6cQ4dkJVg"}},{"cell_type":"code","source":["# Analyze cluster distribution\n","unique, counts = np.unique(dbscan_clusters, return_counts=True)  # Get unique cluster labels and their counts from the DBSCAN results\n","cluster_distribution = dict(zip(unique, counts))  # Create a dictionary of cluster labels and their counts\n","print(f'Cluster Distribution: {cluster_distribution}')  # Print the distribution of clusters\n","\n","# Add clusters to the dataframe\n","data['cluster'] = dbscan_clusters  # Add the cluster labels to the original dataframe as a new column named 'cluster'\n","\n","# Analyze properties of each cluster\n","cluster_summary = data.groupby('cluster').mean()  # Calculate the mean of each feature for each cluster\n","print(cluster_summary)  # Print the summary of cluster properties\n","\n","# Visualize distributions of features within each cluster\n","for feature in features:  # Loop through each feature\n","    plt.figure(figsize=(10, 6))  # Set the size of the figure for the plot\n","    sns.histplot(data=data, x=feature, hue='cluster', multiple='stack', palette='viridis', kde=True)  # Create a histogram of the feature values, colored by cluster, with KDE overlay\n","    plt.title(f'Distribution of {feature} by Cluster')  # Set the title of the plot\n","    plt.show()  # Display the histogram"],"metadata":{"id":"GNv6WiNdkVnu"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["\n","### Explanation\n","\n","1. **Analyze Cluster Distribution**:\n","   - `unique, counts = np.unique(dbscan_clusters, return_counts=True)`: This line extracts unique cluster labels and their frequencies from the DBSCAN clustering results.\n","   - `cluster_distribution = dict(zip(unique, counts))`: A dictionary is created mapping each unique cluster label to its count.\n","   - `print(f'Cluster Distribution: {cluster_distribution}')`: The distribution of clusters is printed, showing how many data points are assigned to each cluster.\n","\n","2. **Add Clusters to the DataFrame**:\n","   - `data['cluster'] = dbscan_clusters`: The cluster labels are added to the original DataFrame as a new column named `'cluster'`, allowing for easy access to cluster assignments alongside the feature data.\n","\n","3. **Analyze Properties of Each Cluster**:\n","   - `cluster_summary = data.groupby('cluster').mean()`: This line groups the data by cluster and calculates the mean value of each feature within each cluster.\n","   - `print(cluster_summary)`: The mean values for each feature by cluster are printed, summarizing the properties of each cluster.\n","\n","4. **Visualize Distributions of Features within Each Cluster**:\n","   - `for feature in features`: This loop iterates over each feature specified in the `features` list.\n","   - `plt.figure(figsize=(10, 6))`: Sets the size of the figure for the histogram plots.\n","   - `sns.histplot(data=data, x=feature, hue='cluster', multiple='stack', palette='viridis', kde=True)`: Creates a stacked histogram for each feature, with bars colored according to cluster labels. `kde=True` adds a kernel density estimate overlay to the histogram.\n","   - `plt.title(f'Distribution of {feature} by Cluster')`: Sets the title of the histogram plot for the current feature.\n","   - `plt.show()`: Displays the histogram for the current feature, providing a visual representation of feature distributions across clusters."],"metadata":{"id":"41_EXPthkmuK"}},{"cell_type":"code","source":["import tensorflow as tf  # Import TensorFlow library\n","from tensorflow.keras.models import Model  # Import Model class from Keras\n","from tensorflow.keras.layers import Input, Dense  # Import Input and Dense layers from Keras\n","\n","# Define the autoencoder model\n","input_dim = data_scaled.shape[1]  # Set the input dimension based on the number of features\n","encoding_dim = 2  # Define the dimension of the encoded representation\n","\n","input_layer = Input(shape=(input_dim,))  # Define the input layer with the shape of the input data\n","encoder = Dense(encoding_dim, activation='relu')(input_layer)  # Define the encoder layer with ReLU activation\n","decoder = Dense(input_dim, activation='sigmoid')(encoder)  # Define the decoder layer with sigmoid activation\n","\n","autoencoder = Model(inputs=input_layer, outputs=decoder)  # Create the autoencoder model\n","encoder_model = Model(inputs=input_layer, outputs=encoder)  # Create a separate encoder model\n","\n","autoencoder.compile(optimizer='adam', loss='mse')  # Compile the autoencoder model with Adam optimizer and MSE loss\n","\n","# Train the autoencoder\n","autoencoder.fit(data_scaled, data_scaled, epochs=50, batch_size=32, shuffle=True, validation_split=0.2, verbose=0)  # Train the autoencoder model\n","\n","# Encode the data\n","encoded_data = encoder_model.predict(data_scaled)  # Use the encoder model to encode the data\n","\n","# K-Means Clustering on encoded data\n","kmeans_encoded = KMeans(n_clusters=3, random_state=42, n_init='auto')  # Define K-Means clustering with 3 clusters\n","clusters_encoded = kmeans_encoded.fit_predict(encoded_data)  # Perform K-Means clustering on the encoded data\n","\n","# Evaluation of Clusters on encoded data\n","silhouette_avg_encoded = silhouette_score(encoded_data, clusters_encoded)  # Calculate the silhouette score for the clustering\n","print(f'Autoencoder + K-Means Silhouette Score: {silhouette_avg_encoded}')  # Print the silhouette score\n","\n","# Visualization of Clusters on encoded data\n","plt.figure(figsize=(10, 7))  # Set the size of the figure\n","sns.scatterplot(x=encoded_data[:, 0], y=encoded_data[:, 1], hue=clusters_encoded, palette='viridis', s=100, alpha=0.7)  # Scatter plot of the encoded data colored by cluster\n","plt.title('Autoencoder + K-Means Clustering of Neutron Stars')  # Set the title of the plot\n","plt.xlabel('Encoded Dimension 1')  # Set the x-axis label\n","plt.ylabel('Encoded Dimension 2')  # Set the y-axis label\n","plt.show()  # Display the scatter plot"],"metadata":{"id":"SMmOuAtEkF-I"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["\n","### Explanation\n","\n","1. **Import Libraries**:\n","   - `import tensorflow as tf`: Imports TensorFlow library for building and training neural networks.\n","   - `from tensorflow.keras.models import Model`: Imports the Model class for creating neural network models.\n","   - `from tensorflow.keras.layers import Input, Dense`: Imports the Input and Dense layers for building the autoencoder.\n","\n","2. **Define the Autoencoder Model**:\n","   - `input_dim = data_scaled.shape[1]`: Gets the number of features (input dimensions) from the scaled data.\n","   - `encoding_dim = 2`: Sets the number of dimensions for the encoded representation.\n","   - `input_layer = Input(shape=(input_dim,))`: Defines the input layer with the shape of the input data.\n","   - `encoder = Dense(encoding_dim, activation='relu')(input_layer)`: Defines the encoder layer with ReLU activation function, reducing dimensions to `encoding_dim`.\n","   - `decoder = Dense(input_dim, activation='sigmoid')(encoder)`: Defines the decoder layer with sigmoid activation function, reconstructing the input data.\n","\n","3. **Compile and Train the Autoencoder**:\n","   - `autoencoder = Model(inputs=input_layer, outputs=decoder)`: Creates the autoencoder model with the specified input and output layers.\n","   - `encoder_model = Model(inputs=input_layer, outputs=encoder)`: Creates a separate encoder model to extract the encoded representation.\n","   - `autoencoder.compile(optimizer='adam', loss='mse')`: Compiles the autoencoder model using the Adam optimizer and mean squared error loss.\n","   - `autoencoder.fit(...)`: Trains the autoencoder model using the scaled data for 50 epochs with a batch size of 32, including a validation split.\n","\n","4. **Encode the Data**:\n","   - `encoded_data = encoder_model.predict(data_scaled)`: Encodes the scaled data using the trained encoder model.\n","\n","5. **K-Means Clustering on Encoded Data**:\n","   - `kmeans_encoded = KMeans(n_clusters=3, random_state=42, n_init='auto')`: Initializes K-Means clustering with 3 clusters.\n","   - `clusters_encoded = kmeans_encoded.fit_predict(encoded_data)`: Performs clustering on the encoded data and assigns cluster labels.\n","\n","6. **Evaluate Clustering Results**:\n","   - `silhouette_avg_encoded = silhouette_score(encoded_data, clusters_encoded)`: Calculates the silhouette score for the clustering to evaluate its quality.\n","   - `print(f'Autoencoder + K-Means Silhouette Score: {silhouette_avg_encoded}')`: Prints the silhouette score for the clustering.\n","\n","7. **Visualize Clusters**:\n","   - `plt.figure(figsize=(10, 7))`: Sets the figure size for the scatter plot.\n","   - `sns.scatterplot(...)`: Creates a scatter plot of the encoded data with colors indicating different clusters.\n","   - `plt.title('Autoencoder + K-Means Clustering of Neutron Stars')`: Sets the plot title.\n","   - `plt.xlabel('Encoded Dimension 1')` and `plt.ylabel('Encoded Dimension 2')`: Label the axes of the scatter plot.\n","   - `plt.show()`: Displays the scatter plot showing the clustering results on the encoded data."],"metadata":{"id":"uBkNF0Ojk_xh"}},{"cell_type":"code","source":["from sklearn.cluster import DBSCAN  # Import the DBSCAN clustering algorithm from scikit-learn\n","\n","# Apply DBSCAN on the encoded data\n","dbscan_encoded = DBSCAN(eps=0.5, min_samples=5)  # Initialize DBSCAN with epsilon (eps) and minimum samples parameters\n","clusters_encoded_dbscan = dbscan_encoded.fit_predict(encoded_data)  # Fit DBSCAN on encoded data and predict clusters\n","\n","# Evaluation of DBSCAN Clusters on encoded data\n","silhouette_avg_encoded_dbscan = silhouette_score(encoded_data, clusters_encoded_dbscan)  # Compute silhouette score for DBSCAN clusters\n","print(f'Autoencoder + DBSCAN Silhouette Score: {silhouette_avg_encoded_dbscan}')  # Print the silhouette score\n","\n","# Visualizing the DBSCAN clusters on encoded data\n","plt.figure(figsize=(10, 7))  # Set the size of the figure\n","sns.scatterplot(x=encoded_data[:, 0], y=encoded_data[:, 1], hue=clusters_encoded_dbscan, palette='viridis', s=100, alpha=0.7)  # Scatter plot of encoded data with DBSCAN clusters\n","plt.title('Autoencoder + DBSCAN Clustering of Neutron Stars')  # Set the title of the plot\n","plt.xlabel('Encoded Dimension 1')  # Set the x-axis label\n","plt.ylabel('Encoded Dimension 2')  # Set the y-axis label\n","plt.show()  # Display the scatter plot\n","\n","# Analyze cluster distribution\n","unique_encoded_dbscan, counts_encoded_dbscan = np.unique(clusters_encoded_dbscan, return_counts=True)  # Count occurrences of each cluster label\n","cluster_distribution_encoded_dbscan = dict(zip(unique_encoded_dbscan, counts_encoded_dbscan))  # Create a dictionary of cluster distribution\n","print(f'Cluster Distribution (Autoencoder + DBSCAN): {cluster_distribution_encoded_dbscan}')  # Print the cluster distribution\n","\n","# Add clusters to the dataframe\n","data['cluster_encoded_dbscan'] = clusters_encoded_dbscan  # Add the DBSCAN cluster labels to the dataframe\n","\n","# Analyze properties of each cluster\n","cluster_summary_encoded_dbscan = data.groupby('cluster_encoded_dbscan').mean()  # Compute mean values of features for each cluster\n","print(cluster_summary_encoded_dbscan)  # Print the cluster summary\n","\n","# Following code is commented out for faster execution, un-comment to visualize distributions of features within each cluster\n","\"\"\"for feature in features:\n","    plt.figure(figsize=(10, 6))\n","    sns.histplot(data=data, x=feature, hue='cluster_encoded_dbscan', multiple='stack', palette='viridis', kde=True)\n","    plt.title(f'Distribution of {feature} by Autoencoder + DBSCAN Cluster')\n","    plt.show()\"\"\"  # (Commented out) Code to visualize feature distributions within each DBSCAN cluster"],"metadata":{"id":"8XKRiMVOlK1G"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["\n","### Explanation\n","\n","1. **Apply DBSCAN on Encoded Data**:\n","   - `dbscan_encoded = DBSCAN(eps=0.5, min_samples=5)`: Initializes the DBSCAN algorithm with `eps` (maximum distance between samples) and `min_samples` (minimum number of samples for a cluster).\n","   - `clusters_encoded_dbscan = dbscan_encoded.fit_predict(encoded_data)`: Applies DBSCAN clustering to the encoded data and assigns cluster labels.\n","\n","2. **Evaluate Clustering Results**:\n","   - `silhouette_avg_encoded_dbscan = silhouette_score(encoded_data, clusters_encoded_dbscan)`: Calculates the silhouette score for DBSCAN clusters, which measures the quality of clustering.\n","   - `print(f'Autoencoder + DBSCAN Silhouette Score: {silhouette_avg_encoded_dbscan}')`: Prints the silhouette score to evaluate clustering performance.\n","\n","3. **Visualize DBSCAN Clusters**:\n","   - `plt.figure(figsize=(10, 7))`: Sets the size of the figure for visualization.\n","   - `sns.scatterplot(...)`: Creates a scatter plot of the encoded data, coloring points by their DBSCAN cluster labels.\n","   - `plt.title('Autoencoder + DBSCAN Clustering of Neutron Stars')`: Adds a title to the plot.\n","   - `plt.xlabel('Encoded Dimension 1')` and `plt.ylabel('Encoded Dimension 2')`: Labels the axes of the scatter plot.\n","   - `plt.show()`: Displays the scatter plot showing DBSCAN clustering results.\n","\n","4. **Analyze Cluster Distribution**:\n","   - `unique_encoded_dbscan, counts_encoded_dbscan = np.unique(clusters_encoded_dbscan, return_counts=True)`: Counts the number of occurrences of each cluster label.\n","   - `cluster_distribution_encoded_dbscan = dict(zip(unique_encoded_dbscan, counts_encoded_dbscan))`: Creates a dictionary to represent the distribution of clusters.\n","   - `print(f'Cluster Distribution (Autoencoder + DBSCAN): {cluster_distribution_encoded_dbscan}')`: Prints the distribution of clusters.\n","\n","5. **Add Clusters to DataFrame**:\n","   - `data['cluster_encoded_dbscan'] = clusters_encoded_dbscan`: Adds the cluster labels from DBSCAN to the original dataframe for further analysis.\n","\n","6. **Analyze Cluster Properties**:\n","   - `cluster_summary_encoded_dbscan = data.groupby('cluster_encoded_dbscan').mean()`: Computes the average values of features for each DBSCAN cluster.\n","   - `print(cluster_summary_encoded_dbscan)`: Prints the summary of cluster properties.\n","\n","7. **(Commented Out) Visualize Feature Distributions**:\n","   - The commented-out code would plot histograms of feature distributions for each DBSCAN cluster, which can be uncommented if needed for detailed analysis."],"metadata":{"id":"7x773j9HlcTo"}}]}